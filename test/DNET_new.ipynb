{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "71f3B6CWaroo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "\n",
    "\n",
    "# numpy.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPeh3HMaVeR_",
    "outputId": "0099c36f-3986-40f5-e3ab-759a2048d221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: Sequences \n",
      "y: Labels \n",
      "\n",
      "x\n",
      "y\n",
      "('\\nSequences: ', (8920, 1182), '\\nLabels', (8920,))\n"
     ]
    }
   ],
   "source": [
    "# Decription of the dataset\n",
    "data = load('dataset.npz')\n",
    "lst = data.files\n",
    "\n",
    "print(\"x: Sequences \\ny: Labels \\n\")\n",
    "\n",
    "for item in lst:\n",
    "    print(item)\n",
    "\n",
    "print('\\nSequences: ',data['x'].shape,'\\nLabels', data['y'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "H07g7P_Ibo8O"
   },
   "outputs": [],
   "source": [
    "# temp = torch.zeros((16,16), dtype=torch.int)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EFvxYncebUb7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    One hot encoding function\n",
    "    args: 'data'\n",
    "            Type: a single np array (1182, 16)\n",
    "    return: 'New Tensor'\n",
    "            Takes the np array, finds the one hot encoding char\n",
    "            Creates a new tensor and returns it (1182, 16, 16)\n",
    "'''\n",
    "def one_hot_encoded(data): #1182*16 to 1182*16*16\n",
    "  new_tensor = torch.zeros((1182, 16,16))\n",
    "  for i in range(data.shape[0]):\n",
    "    row,col=data[i],data[i]\n",
    "    new_tensor[i,row,col]=1\n",
    "  return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8Tfpkj_imbL",
    "outputId": "0542b0d9-5be0-42d6-d457-587fa5596dad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/8920 [00:00<03:58, 37.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8920, 1182, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8920/8920 [03:47<00:00, 39.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8920, 1182, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert all the sequences to one hot encoded matrices of size 16*16\n",
    "\n",
    "# Load the sequences\n",
    "new_data = data['x'].astype(int)\n",
    "np.random.shuffle(new_data)\n",
    "sequences_n = torch.zeros(new_data.shape[0], new_data.shape[1], 16,16)\n",
    "print(sequences_n.size())\n",
    "\n",
    "# Pass the sequences to one hot encoding function\n",
    "for i in tqdm(range(new_data.shape[0])):\n",
    "    sequences_n[i]=one_hot_encoded(new_data[i])\n",
    "#     if(i%200 ==0):\n",
    "#         print(i,' hogae')\n",
    "print(sequences_n.size())\n",
    "\n",
    "# Load and convert the labels to torch tensor\n",
    "labels = torch.from_numpy(data['y'].astype(long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6PdspGWS7Er",
    "outputId": "e9db754d-f435-4feb-cb1c-7a9fdad81e54"
   },
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "full_dataset = TensorDataset(sequences_n, labels)\n",
    "\n",
    "# Split the dataset\n",
    "train_ds, test_ds = torch.utils.data.random_split(full_dataset, (6320, 2600))\n",
    "\n",
    "# Print and confirm the dataset\n",
    "# print(train_ds, test_ds)\n",
    "# print(len(train_ds.indices), len(test_ds))\n",
    "# print(train_ds.indices, test_ds.indices)\n",
    "\n",
    "# Create the train data loader\n",
    "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n",
    "\n",
    "# Create the test data loader\n",
    "test_dataloader = DataLoader(test_ds, batch_size=32, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bia5Q7vkxYQI"
   },
   "source": [
    "# Creation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "EDK1AZAjiuhi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "    Type: RCPClass\n",
    "    The class conatains the layers of the Dense and Transition blocks\n",
    "    \n",
    "    Args: Takes the dataset (batch size, sequence_size, w, h)\n",
    "    return: Logits (Probabilities of the all 13 classes)\n",
    "'''\n",
    "\n",
    "# Create the RCPNet Class\n",
    "class RPCNet(nn.Module):\n",
    "  def __init__(self, in_channels):\n",
    "    super(RPCNet, self).__init__()\n",
    "    \n",
    "    self.in_channels = in_channels\n",
    "    self.conv1 = nn.Conv2d(in_channels=1182, out_channels=128, kernel_size=(16,17), padding=9)\n",
    "    \n",
    "    # First Dense Net Block\n",
    "    self.DenseBlock1 = DenseBlock(128)\n",
    "    \n",
    "    # Second Dense Net Block\n",
    "    self.DenseBlock2 = DenseBlock(256)\n",
    "    \n",
    "    # Third Dense Net Block\n",
    "    self.DenseBlock3 = DenseBlock(512)\n",
    "    \n",
    "    # Transition Block 1\n",
    "    self.pos_dense1 = nn.Sequential(\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(in_channels=128,out_channels=256,kernel_size=(1,1)),\n",
    "          nn.MaxPool2d(1,4))\n",
    "    \n",
    "    # Transition Block 2\n",
    "    self.pos_dense2 = nn.Sequential(\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(in_channels=256,out_channels=512,kernel_size=(1,1)),\n",
    "          nn.MaxPool2d(1,4))\n",
    "    \n",
    "    # Transition Block 3\n",
    "    self.pos_dense3 = nn.Sequential(\n",
    "          nn.BatchNorm2d(512),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=(1,1)),\n",
    "          nn.MaxPool2d(1,4))\n",
    "    \n",
    "    # Final linear layer, can be used with a Flatten Layer \n",
    "    self.f= nn.Flatten()\n",
    "    self.Linear = nn.Linear(22528, 13)\n",
    "      \n",
    "  def forward(self, input):\n",
    "    # input = input.reshape(input.size(0), input.size(2), input.size(1))\n",
    "#     print(input.size())\n",
    "    out = self.conv1(input)\n",
    "#     print(out.size())\n",
    "    \n",
    "#     print(\"conv1 shape\", out.size())\n",
    "    weight = self.conv1.weight.data.cpu().numpy()\n",
    "#     print(\"k:\",weight.shape)\n",
    "\n",
    "    out= self.DenseBlock1(out)\n",
    "\n",
    "    out= self.pos_dense1(out)\n",
    "#     print(out.size())\n",
    "\n",
    "    out= self.DenseBlock2(out)\n",
    "#     print(out.size())\n",
    "    \n",
    "    out = self.pos_dense2(out)\n",
    "#     print(out.size())\n",
    "    \n",
    "    out= self.DenseBlock3(out)\n",
    "#     print(out.size())\n",
    "    \n",
    "    out = self.pos_dense3(out)\n",
    "#     print(out.size())\n",
    "    \n",
    "    logits = self.f(out)\n",
    "    \n",
    "    logits= self.Linear(logits)\n",
    "#     print('logits size',logits.size())\n",
    "#     print(logits)\n",
    "\n",
    "    return logits\n",
    "\n",
    "'''\n",
    "    Type: Class DenseBlock\n",
    "          Contains Batch Normalization, ReLU, Conv2D\n",
    "          \n",
    "    Input: Declaration: Num of input channels \n",
    "           Forward: Output of previous layer\n",
    "    \n",
    "    Can be used before transition blocks or other layers\n",
    "'''\n",
    "class DenseBlock(nn.Module):\n",
    "  def __init__(self, num_features):\n",
    "    super(DenseBlock, self).__init__()\n",
    "\n",
    "    self.Block1 = Block(num_features) # applies relu and BN\n",
    "    self.Block2 = Block(num_features)\n",
    "\n",
    "  def forward(self, o_input):\n",
    "\n",
    "    block1_res = self.Block1(o_input)\n",
    "#     print(\"i and c in b \",block1_res.size(), o_input.size())\n",
    "    block2_res = (torch.cat((block1_res, o_input), dim=2)) # o_inpu (concat) block1res\n",
    "#     print(block2_res.size())\n",
    "#     res = torch.cat((block1_res, block2_res, o_input), dim=2)\n",
    "#     print(res.size())\n",
    "    return  torch.cat((block1_res, block2_res, o_input), dim=2) # concat, block1_res, block2_res, o_input\n",
    "\n",
    "'''\n",
    "    Type: Block Class\n",
    "    \n",
    "    Input: Declaration: Num of input channels \n",
    "           Forward: Output of the previous layer\n",
    "           \n",
    "    return: Concatenated input and output of the BN, ReLU, Conv2D\n",
    "'''\n",
    "\n",
    "class Block(nn.Module):\n",
    "  def __init__(self, num_features):\n",
    "    super(Block, self).__init__()\n",
    "    self.num_features = num_features\n",
    "\n",
    "    self.BN = nn.BatchNorm2d(self.num_features) # try 2d\n",
    "    self.conv1 = nn.Conv2d(in_channels=self.num_features, out_channels=num_features, kernel_size=(16,17), padding=8)\n",
    "  \n",
    "  def forward(self, input):\n",
    "    res_relu = f.relu(self.BN(input))\n",
    "    res_relu_conv = self.conv1(input)\n",
    "#     print(\"w c block\",self.conv1.weight.data.cpu().numpy().shape)\n",
    "\n",
    "    return res_relu_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializaiton of Model, Optimizer, Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "QCPdl3wT9VPC"
   },
   "outputs": [],
   "source": [
    "model = RPCNet(1182)\n",
    "model=model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w70Z1THM1H59",
    "outputId": "59c8053b-8905-489f-f509-0b177dbe98ce"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 272.00 MiB (GPU 0; 31.74 GiB total capacity; 2.10 GiB already allocated; 183.19 MiB free; 2.18 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c53e0bd4fe01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtotal_loss_i\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ahtisham/.local/lib/python2.7/site-packages/torch/optim/adam.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0;31m# Exponential moving average of squared gradient values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                     \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exp_avg_sq'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                         \u001b[0;31m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 272.00 MiB (GPU 0; 31.74 GiB total capacity; 2.10 GiB already allocated; 183.19 MiB free; 2.18 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "total_loss_e = 0.0\n",
    "\n",
    "plot_loss = []\n",
    "for epoch in range(50):\n",
    "    total_loss_i = 0\n",
    "    total=0.0\n",
    "    correct=0.0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        x,y=batch\n",
    "        logits = model(x.cuda())\n",
    "        loss = loss_function(logits,y.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_i += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted.cpu() == y).sum().item()\n",
    "        if i%50 ==0:\n",
    "            print(\"Iter : {}  Train Loss : {}\".format(i,loss.item()))\n",
    "    total_loss_e+=total_loss_i/len(train_dataloader)\n",
    "    print(\"Epoch  {} :   Train Loss : {}  Accuracy : {}\".format(epoch, total_loss_e, (100 * (correct / total))))\n",
    "    plot_loss.append(total_loss_e)\n",
    "    total_loss_e=0\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "Y_-KiuH9911J"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(plot_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0  Test Loss : 2.94496107101\n",
      "Iter : 1  Test Loss : 2.45704984665\n",
      "Iter : 2  Test Loss : 2.03724956512\n",
      "Iter : 3  Test Loss : 2.57082843781\n",
      "Iter : 4  Test Loss : 5.34302186966\n",
      "Iter : 5  Test Loss : 1.75534343719\n",
      "Iter : 6  Test Loss : 1.78849947453\n",
      "Iter : 7  Test Loss : 3.88453674316\n",
      "Iter : 8  Test Loss : 2.37283420563\n",
      "Iter : 9  Test Loss : 0.776594400406\n",
      "Iter : 10  Test Loss : 5.25816011429\n",
      "Iter : 11  Test Loss : 1.26096749306\n",
      "Iter : 12  Test Loss : 3.16063666344\n",
      "Iter : 13  Test Loss : 3.2196290493\n",
      "Iter : 14  Test Loss : 2.58067417145\n",
      "Iter : 15  Test Loss : 2.39291763306\n",
      "Iter : 16  Test Loss : 2.45247483253\n",
      "Iter : 17  Test Loss : 2.16455054283\n",
      "Iter : 18  Test Loss : 2.19094610214\n",
      "Iter : 19  Test Loss : 2.048869133\n",
      "Iter : 20  Test Loss : 3.94749498367\n",
      "Iter : 21  Test Loss : 1.6768206358\n",
      "Iter : 22  Test Loss : 1.79274606705\n",
      "Iter : 23  Test Loss : 2.600055933\n",
      "Iter : 24  Test Loss : 4.39622020721\n",
      "Iter : 25  Test Loss : 2.1419699192\n",
      "Iter : 26  Test Loss : 1.97283899784\n",
      "Iter : 27  Test Loss : 2.50723648071\n",
      "Iter : 28  Test Loss : 2.83373165131\n",
      "Iter : 29  Test Loss : 4.01042556763\n",
      "Iter : 30  Test Loss : 1.93126702309\n",
      "Iter : 31  Test Loss : 2.10212492943\n",
      "Iter : 32  Test Loss : 1.87583470345\n",
      "Iter : 33  Test Loss : 2.10917448997\n",
      "Iter : 34  Test Loss : 3.12763953209\n",
      "Iter : 35  Test Loss : 1.67030858994\n",
      "Iter : 36  Test Loss : 3.53262114525\n",
      "Iter : 37  Test Loss : 2.36229538918\n",
      "Iter : 38  Test Loss : 2.93284916878\n",
      "Iter : 39  Test Loss : 1.68035924435\n",
      "Iter : 40  Test Loss : 1.74189507961\n",
      "Iter : 41  Test Loss : 1.2951823473\n",
      "Iter : 42  Test Loss : 1.25837206841\n",
      "Iter : 43  Test Loss : 3.03675603867\n",
      "Iter : 44  Test Loss : 0.61016112566\n",
      "Iter : 45  Test Loss : 1.95963704586\n",
      "Iter : 46  Test Loss : 3.92857336998\n",
      "Iter : 47  Test Loss : 2.44339394569\n",
      "Iter : 48  Test Loss : 0.966798007488\n",
      "Iter : 49  Test Loss : 1.14876639843\n",
      "Iter : 50  Test Loss : 0.98660248518\n",
      "Iter : 51  Test Loss : 2.2427110672\n",
      "Iter : 52  Test Loss : 1.8940217495\n",
      "Iter : 53  Test Loss : 2.55012249947\n",
      "Iter : 54  Test Loss : 2.63824510574\n",
      "Iter : 55  Test Loss : 2.96804404259\n",
      "Iter : 56  Test Loss : 2.61360836029\n",
      "Iter : 57  Test Loss : 1.67857384682\n",
      "Iter : 58  Test Loss : 4.36670875549\n",
      "Iter : 59  Test Loss : 1.74821519852\n",
      "Iter : 60  Test Loss : 2.72262907028\n",
      "Iter : 61  Test Loss : 3.17984676361\n",
      "Iter : 62  Test Loss : 1.38008284569\n",
      "Iter : 63  Test Loss : 1.7200217247\n",
      "Iter : 64  Test Loss : 2.72861289978\n",
      "Iter : 65  Test Loss : 2.42266631126\n",
      "Iter : 66  Test Loss : 2.41111946106\n",
      "Iter : 67  Test Loss : 1.10315811634\n",
      "Iter : 68  Test Loss : 2.28860735893\n",
      "Iter : 69  Test Loss : 2.9467446804\n",
      "Iter : 70  Test Loss : 1.96052312851\n",
      "Iter : 71  Test Loss : 1.35037541389\n",
      "Iter : 72  Test Loss : 2.26676988602\n",
      "Iter : 73  Test Loss : 0.978027701378\n",
      "Iter : 74  Test Loss : 2.597884655\n",
      "Iter : 75  Test Loss : 2.57438516617\n",
      "Iter : 76  Test Loss : 1.85042774677\n",
      "Iter : 77  Test Loss : 4.92865943909\n",
      "Iter : 78  Test Loss : 2.4026632309\n",
      "Iter : 79  Test Loss : 3.08066225052\n",
      "Iter : 80  Test Loss : 3.69449090958\n",
      "Iter : 81  Test Loss : 2.90404558182\n",
      " Accuracy : 71.9230769231\n"
     ]
    }
   ],
   "source": [
    "total_loss_i = 0\n",
    "total=0.0\n",
    "correct=0.0\n",
    "model.eval()\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    x,y=batch\n",
    "    logits = model(x.cuda())\n",
    "    loss = loss_function(logits,y.cuda())\n",
    "    total_loss_i += loss.item()\n",
    "    _, predicted = torch.max(logits.data, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted.cpu() == y).sum().item()\n",
    "    print(\"Iter : {}  Test Loss : {}\".format(i,loss.item()))\n",
    "    \n",
    "# total_loss_e+=total_loss_i/len(test_dataloader)\n",
    "print(\" Accuracy : {}\".format( (100 * (correct / total))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ground Truth:', tensor([ 4,  6,  3,  8,  2,  0,  9,  5,  6, 10, 11,  6,  6,  1,  3,  8,  6, 12,\n",
      "         3,  0,  9,  2, 12,  0,  5,  5,  5,  4,  5,  0,  1, 11]))\n",
      "('Predictions', torch.return_types.max(\n",
      "values=tensor([14.4684, 85.1889,  9.1602,  8.8477,  4.4606, 36.2412,  6.6645, 22.4495,\n",
      "        47.1942, 22.3329, 17.2882, 66.4394, 88.0309, 80.9064, 14.3579,  5.3693,\n",
      "        91.5169, 37.7294, 11.4029, 55.8283, 18.3636, 18.2256, 37.0522, 48.3913,\n",
      "        11.2702, 14.9665,  8.7802, 16.0170, 16.6690, 35.1423, 78.4830, 23.1582],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>),\n",
      "indices=tensor([ 4,  6,  8,  8, 11,  0,  4,  5,  6,  4, 11,  6,  6,  1,  3,  2,  6, 12,\n",
      "         3,  0, 10,  2, 12,  0,  5,  4,  5,  9,  5,  0,  1, 11],\n",
      "       device='cuda:0')))\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(test_dataloader))\n",
    "print(\"Ground Truth:\", y)\n",
    "new_arr = x.cuda()\n",
    "preds = model(new_arr)\n",
    "print(\"Predictions\", torch.max(preds, dim =1 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled27.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
