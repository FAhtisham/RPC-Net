{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "71f3B6CWaroo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPeh3HMaVeR_",
    "outputId": "0099c36f-3986-40f5-e3ab-759a2048d221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: Sequences \n",
      "y: Labels \n",
      "\n",
      "x\n",
      "y\n",
      "('\\nSequences: ', (8920, 1182), '\\nLabels', (8920,))\n"
     ]
    }
   ],
   "source": [
    "# Decription of the dataset\n",
    "data = load('dataset.npz')\n",
    "lst = data.files\n",
    "\n",
    "print(\"x: Sequences \\ny: Labels \\n\")\n",
    "\n",
    "for item in lst:\n",
    "    print(item)\n",
    "\n",
    "print('\\nSequences: ',data['x'].shape,'\\nLabels', data['y'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H07g7P_Ibo8O"
   },
   "outputs": [],
   "source": [
    "# temp = torch.zeros((16,16), dtype=torch.int)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EFvxYncebUb7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    One hot encoding function\n",
    "    args: 'data'\n",
    "            Type: a single np array (1182, 16)\n",
    "    return: 'New Tensor'\n",
    "            Takes the np array, finds the one hot encoding char\n",
    "            Creates a new tensor and returns it (1182, 16, 16)\n",
    "'''\n",
    "def one_hot_encoded(data): #1182*16 to 1182*16*16\n",
    "  new_tensor = torch.zeros((1182, 16,16))\n",
    "  for i in range(data.shape[0]):\n",
    "    row,col=data[i],data[i]\n",
    "    new_tensor[i,row,col]=1\n",
    "  return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8Tfpkj_imbL",
    "outputId": "0542b0d9-5be0-42d6-d457-587fa5596dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8920, 1182, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "# Convert all the sequences to one hot encoded matrices of size 16*16\n",
    "\n",
    "# Load the sequences\n",
    "new_data = data['x'].astype(int)\n",
    "np.random.shuffle(new_data)\n",
    "sequences_n = torch.zeros(new_data.shape[0], new_data.shape[1], 16,16)\n",
    "\n",
    "# Pass the sequences to one hot encoding function\n",
    "for i in range(new_data.shape[0]):\n",
    "    sequences_n[i]=one_hot_encoded(new_data[i])\n",
    "print(sequences_n.size())\n",
    "\n",
    "# Load and convert the labels to torch tensor\n",
    "labels = torch.from_numpy(data['y'].astype(long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6PdspGWS7Er",
    "outputId": "e9db754d-f435-4feb-cb1c-7a9fdad81e54"
   },
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "full_dataset = TensorDataset(sequences_n, labels)\n",
    "\n",
    "# Split the dataset\n",
    "train_ds, test_ds = torch.utils.data.random_split(full_dataset, (6320, 2600))\n",
    "\n",
    "# Print and confirm the dataset\n",
    "# print(train_ds, test_ds)\n",
    "# print(len(train_ds.indices), len(test_ds))\n",
    "# print(train_ds.indices, test_ds.indices)\n",
    "\n",
    "# Create the train data loader\n",
    "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n",
    "\n",
    "# Create the test data loader\n",
    "test_dataloader = DataLoader(test_ds, batch_size=32, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bia5Q7vkxYQI"
   },
   "source": [
    "# Creation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "EDK1AZAjiuhi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "    Type: RCPClass\n",
    "    The class conatains the layers of the Dense and Transition blocks\n",
    "    \n",
    "    Args: Takes the dataset (batch size, sequence_size, w, h)\n",
    "    return: Logits (Probabilities of the all 13 classes)\n",
    "'''\n",
    "\n",
    "# Create the RCPNet Class\n",
    "class RPCNet(nn.Module):\n",
    "  def __init__(self, in_channels):\n",
    "    super(RPCNet, self).__init__()\n",
    "    \n",
    "    self.in_channels = in_channels\n",
    "    self.conv1 = nn.Conv2d(in_channels=1182, out_channels=128, kernel_size=(16,17), padding=(4,5))\n",
    "    \n",
    "    # First Dense Net Block\n",
    "    self.DenseBlock1 = DenseBlock(128)\n",
    "    \n",
    "    # Second Dense Net Block\n",
    "    self.DenseBlock2 = DenseBlock(256)\n",
    "    \n",
    "    # Third Dense Net Block\n",
    "    self.DenseBlock3 = DenseBlock(512)\n",
    "    \n",
    "    # Transition Block 1\n",
    "    self.pos_dense1 = nn.Sequential(\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(in_channels=128,out_channels=256,kernel_size=(1,1)),\n",
    "          nn.MaxPool2d(1,4))\n",
    "    \n",
    "    # Transition Block 2\n",
    "    self.pos_dense2 = nn.Sequential(\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(in_channels=256,out_channels=512,kernel_size=(1,1), stride=2),\n",
    "          nn.MaxPool2d(1,4))\n",
    "    \n",
    "    # Transition Block 3\n",
    "    self.pos_dense3 = nn.Sequential(\n",
    "          nn.BatchNorm2d(512),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=(1,1),stride=3),\n",
    "          nn.MaxPool2d(1,4))\n",
    "    \n",
    "    # Final linear layer, can be used with a Flatten Layer \n",
    "    self.f= nn.Flatten()\n",
    "    self.Linear = nn.Linear(1024, 13)\n",
    "      \n",
    "  def forward(self, input):\n",
    "    out = self.conv1(input)\n",
    "    \n",
    "#     weight = self.conv1.weight.data.cpu().numpy()\n",
    "\n",
    "    out= self.DenseBlock1(out)\n",
    "\n",
    "    out= self.pos_dense1(out)\n",
    "\n",
    "    out= self.DenseBlock2(out)\n",
    "\n",
    "    out = self.pos_dense2(out)\n",
    "\n",
    "    out= self.DenseBlock3(out)\n",
    "    \n",
    "    out = self.pos_dense3(out)\n",
    "    \n",
    "    logits = self.f(out)\n",
    "    \n",
    "    logits= self.Linear(logits)\n",
    "\n",
    "\n",
    "    return logits\n",
    "\n",
    "'''\n",
    "    Type: Class DenseBlock\n",
    "          Contains Batch Normalization, ReLU, Conv2D\n",
    "          \n",
    "    Input: Declaration: Num of input channels \n",
    "           Forward: Output of previous layer\n",
    "    \n",
    "    Can be used before transition blocks or other layers\n",
    "'''\n",
    "class DenseBlock(nn.Module):\n",
    "  def __init__(self, num_features):\n",
    "    super(DenseBlock, self).__init__()\n",
    "\n",
    "    self.Block1 = Block(num_features) # applies relu and BN\n",
    "    self.Block2 = Block(num_features)\n",
    "\n",
    "  def forward(self, o_input):\n",
    "\n",
    "    block1_res = self.Block1(o_input)\n",
    "    block2_res = (torch.cat((block1_res, o_input), dim=2)) # o_inpu (concat) block1res\n",
    "\n",
    "    return  torch.cat((block1_res, block2_res, o_input), dim=2) # concat, block1_res, block2_res, o_input\n",
    "\n",
    "'''\n",
    "    Type: Block Class\n",
    "    \n",
    "    Input: Declaration: Num of input channels \n",
    "           Forward: Output of the previous layer\n",
    "           \n",
    "    return: Concatenated input and output of the BN, ReLU, Conv2D\n",
    "'''\n",
    "\n",
    "class Block(nn.Module):\n",
    "  def __init__(self, num_features):\n",
    "    super(Block, self).__init__()\n",
    "    self.num_features = num_features\n",
    "\n",
    "    self.BN = nn.BatchNorm2d(self.num_features) # try 2d\n",
    "    self.conv1 = nn.Conv2d(in_channels=self.num_features, out_channels=num_features, kernel_size=(16,17), padding=(6,8))\n",
    "  \n",
    "  def forward(self, input):\n",
    "    res_relu = f.relu(self.BN(input))\n",
    "    res_relu_conv = self.conv1(input)\n",
    "#     print(\"w c block\",self.conv1.weight.data.cpu().numpy().shape)\n",
    "\n",
    "    return res_relu_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializaiton of Model, Optimizer, Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "QCPdl3wT9VPC"
   },
   "outputs": [],
   "source": [
    "model = RPCNet(1182)\n",
    "model=model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w70Z1THM1H59",
    "outputId": "59c8053b-8905-489f-f509-0b177dbe98ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0  Train Loss : 2.5713031292\n",
      "Iter : 50  Train Loss : 2.58723950386\n",
      "Iter : 100  Train Loss : 2.60588026047\n",
      "Iter : 150  Train Loss : 2.59415745735\n",
      "Epoch  0 :   Train Loss : 2.61467716911  Accuracy : 7.48417721519\n",
      "Iter : 0  Train Loss : 2.55360889435\n",
      "Iter : 50  Train Loss : 2.5508286953\n",
      "Iter : 100  Train Loss : 2.56761574745\n",
      "Iter : 150  Train Loss : 2.56127548218\n",
      "Epoch  1 :   Train Loss : 2.56952546221  Accuracy : 7.70569620253\n",
      "Iter : 0  Train Loss : 2.55078101158\n",
      "Iter : 50  Train Loss : 2.58636498451\n",
      "Iter : 100  Train Loss : 2.56652545929\n",
      "Iter : 150  Train Loss : 2.55238771439\n",
      "Epoch  2 :   Train Loss : 2.56415418543  Accuracy : 7.56329113924\n",
      "Iter : 0  Train Loss : 2.54021525383\n",
      "Iter : 50  Train Loss : 2.59030914307\n",
      "Iter : 100  Train Loss : 2.56622266769\n",
      "Iter : 150  Train Loss : 2.55183744431\n",
      "Epoch  3 :   Train Loss : 2.56411964364  Accuracy : 7.73734177215\n",
      "Iter : 0  Train Loss : 2.54016566277\n",
      "Iter : 50  Train Loss : 2.59009337425\n",
      "Iter : 100  Train Loss : 2.56610107422\n",
      "Iter : 150  Train Loss : 2.5515666008\n",
      "Epoch  4 :   Train Loss : 2.56399977087  Accuracy : 7.78481012658\n",
      "Iter : 0  Train Loss : 2.54061841965\n",
      "Iter : 50  Train Loss : 2.58985424042\n",
      "Iter : 100  Train Loss : 2.56611514091\n",
      "Iter : 150  Train Loss : 2.55124735832\n",
      "Epoch  5 :   Train Loss : 2.56387872166  Accuracy : 7.8164556962\n",
      "Iter : 0  Train Loss : 2.54101371765\n",
      "Iter : 50  Train Loss : 2.58971452713\n",
      "Iter : 100  Train Loss : 2.56619119644\n",
      "Iter : 150  Train Loss : 2.55091428757\n",
      "Epoch  6 :   Train Loss : 2.56377803918  Accuracy : 7.80063291139\n",
      "Iter : 0  Train Loss : 2.54128813744\n",
      "Iter : 50  Train Loss : 2.58965921402\n",
      "Iter : 100  Train Loss : 2.56626915932\n",
      "Iter : 150  Train Loss : 2.55055880547\n",
      "Epoch  7 :   Train Loss : 2.56369977407  Accuracy : 7.8164556962\n",
      "Iter : 0  Train Loss : 2.54145026207\n",
      "Iter : 50  Train Loss : 2.58962535858\n",
      "Iter : 100  Train Loss : 2.56630635262\n",
      "Iter : 150  Train Loss : 2.55014395714\n",
      "Epoch  8 :   Train Loss : 2.56363864017  Accuracy : 7.91139240506\n",
      "Iter : 0  Train Loss : 2.54151844978\n",
      "Iter : 50  Train Loss : 2.58954644203\n",
      "Iter : 100  Train Loss : 2.56628060341\n",
      "Iter : 150  Train Loss : 2.54963445663\n",
      "Epoch  9 :   Train Loss : 2.56358835553  Accuracy : 7.87974683544\n",
      "Iter : 0  Train Loss : 2.54150652885\n",
      "Iter : 50  Train Loss : 2.58939242363\n",
      "Iter : 100  Train Loss : 2.56619334221\n",
      "Iter : 150  Train Loss : 2.54903411865\n",
      "Epoch  10 :   Train Loss : 2.56354341603  Accuracy : 7.8164556962\n",
      "Iter : 0  Train Loss : 2.54142904282\n",
      "Iter : 50  Train Loss : 2.58918356895\n",
      "Iter : 100  Train Loss : 2.56607413292\n",
      "Iter : 150  Train Loss : 2.54839849472\n",
      "Epoch  11 :   Train Loss : 2.56350039713  Accuracy : 7.91139240506\n",
      "Iter : 0  Train Loss : 2.54131197929\n",
      "Iter : 50  Train Loss : 2.58897781372\n",
      "Iter : 100  Train Loss : 2.56597542763\n",
      "Iter : 150  Train Loss : 2.54781723022\n",
      "Epoch  12 :   Train Loss : 2.56345826448  Accuracy : 7.86392405063\n",
      "Iter : 0  Train Loss : 2.54119491577\n",
      "Iter : 50  Train Loss : 2.58882308006\n",
      "Iter : 100  Train Loss : 2.56594204903\n",
      "Iter : 150  Train Loss : 2.54736042023\n",
      "Epoch  13 :   Train Loss : 2.56341663394  Accuracy : 7.56329113924\n",
      "Iter : 0  Train Loss : 2.54111599922\n",
      "Iter : 50  Train Loss : 2.58875131607\n",
      "Iter : 100  Train Loss : 2.56603550911\n",
      "Iter : 150  Train Loss : 2.54710435867\n",
      "Epoch  14 :   Train Loss : 2.56337768261  Accuracy : 7.75316455696\n",
      "Iter : 0  Train Loss : 2.54113459587\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-66c354b8daa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_loss_e = 0.0\n",
    "\n",
    "plot_loss = []\n",
    "for epoch in range(35):\n",
    "    total_loss_i = 0\n",
    "    total=0.0\n",
    "    correct=0.0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        x,y=batch\n",
    "        logits = model(x.cuda())\n",
    "        loss = loss_function(logits,y.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_i += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted.cpu() == y).sum().item()\n",
    "        if i%50 ==0:\n",
    "            print(\"Iter : {}  Train Loss : {}\".format(i,loss.item()))\n",
    "    total_loss_e+=total_loss_i/len(train_dataloader)\n",
    "    print(\"Epoch  {} :   Train Loss : {}  Accuracy : {}\".format(epoch, total_loss_e, (100 * (correct / total))))\n",
    "    plot_loss.append(total_loss_e)\n",
    "    total_loss_e=0\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "id": "Y_-KiuH9911J"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(plot_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0  Test Loss : 3.13232660294\n",
      "Iter : 1  Test Loss : 2.0720872879\n",
      "Iter : 2  Test Loss : 2.15397286415\n",
      "Iter : 3  Test Loss : 2.55477499962\n",
      "Iter : 4  Test Loss : 2.52595114708\n",
      "Iter : 5  Test Loss : 1.34941959381\n",
      "Iter : 6  Test Loss : 0.605288505554\n",
      "Iter : 7  Test Loss : 3.10459494591\n",
      "Iter : 8  Test Loss : 1.05646944046\n",
      "Iter : 9  Test Loss : 1.77898013592\n",
      "Iter : 10  Test Loss : 3.73812460899\n",
      "Iter : 11  Test Loss : 2.36242675781\n",
      "Iter : 12  Test Loss : 2.8255879879\n",
      "Iter : 13  Test Loss : 2.22934389114\n",
      "Iter : 14  Test Loss : 4.49784231186\n",
      "Iter : 15  Test Loss : 2.48330259323\n",
      "Iter : 16  Test Loss : 1.13705301285\n",
      "Iter : 17  Test Loss : 3.67113828659\n",
      "Iter : 18  Test Loss : 2.26736903191\n",
      "Iter : 19  Test Loss : 2.38937711716\n",
      "Iter : 20  Test Loss : 3.86079454422\n",
      "Iter : 21  Test Loss : 1.9809910059\n",
      "Iter : 22  Test Loss : 2.32371592522\n",
      "Iter : 23  Test Loss : 2.6198015213\n",
      "Iter : 24  Test Loss : 3.16245865822\n",
      "Iter : 25  Test Loss : 2.20257115364\n",
      "Iter : 26  Test Loss : 1.20656847954\n",
      "Iter : 27  Test Loss : 1.1062964201\n",
      "Iter : 28  Test Loss : 3.64655303955\n",
      "Iter : 29  Test Loss : 3.03356575966\n",
      "Iter : 30  Test Loss : 1.49689650536\n",
      "Iter : 31  Test Loss : 1.99918985367\n",
      "Iter : 32  Test Loss : 1.3974006176\n",
      "Iter : 33  Test Loss : 2.7018597126\n",
      "Iter : 34  Test Loss : 1.24141407013\n",
      "Iter : 35  Test Loss : 1.00852739811\n",
      "Iter : 36  Test Loss : 3.47201013565\n",
      "Iter : 37  Test Loss : 3.30791044235\n",
      "Iter : 38  Test Loss : 2.27081131935\n",
      "Iter : 39  Test Loss : 1.74048054218\n",
      "Iter : 40  Test Loss : 1.8883728981\n",
      "Iter : 41  Test Loss : 0.914104759693\n",
      "Iter : 42  Test Loss : 1.6903744936\n",
      "Iter : 43  Test Loss : 1.27614164352\n",
      "Iter : 44  Test Loss : 1.43368387222\n",
      "Iter : 45  Test Loss : 3.38726091385\n",
      "Iter : 46  Test Loss : 2.237844944\n",
      "Iter : 47  Test Loss : 2.09204602242\n",
      "Iter : 48  Test Loss : 1.44501101971\n",
      "Iter : 49  Test Loss : 0.676132321358\n",
      "Iter : 50  Test Loss : 1.55909013748\n",
      "Iter : 51  Test Loss : 1.58667933941\n",
      "Iter : 52  Test Loss : 1.23052334785\n",
      "Iter : 53  Test Loss : 2.13376665115\n",
      "Iter : 54  Test Loss : 2.77069425583\n",
      "Iter : 55  Test Loss : 2.60242724419\n",
      "Iter : 56  Test Loss : 2.91408061981\n",
      "Iter : 57  Test Loss : 1.41122674942\n",
      "Iter : 58  Test Loss : 2.02744007111\n",
      "Iter : 59  Test Loss : 1.10031318665\n",
      "Iter : 60  Test Loss : 4.37346553802\n",
      "Iter : 61  Test Loss : 2.71126484871\n",
      "Iter : 62  Test Loss : 2.5697851181\n",
      "Iter : 63  Test Loss : 2.34673595428\n",
      "Iter : 64  Test Loss : 1.59991014004\n",
      "Iter : 65  Test Loss : 2.77976155281\n",
      "Iter : 66  Test Loss : 0.649642527103\n",
      "Iter : 67  Test Loss : 3.3680460453\n",
      "Iter : 68  Test Loss : 1.0299936533\n",
      "Iter : 69  Test Loss : 1.75304055214\n",
      "Iter : 70  Test Loss : 1.95178723335\n",
      "Iter : 71  Test Loss : 1.81125080585\n",
      "Iter : 72  Test Loss : 2.99854135513\n",
      "Iter : 73  Test Loss : 1.93654060364\n",
      "Iter : 74  Test Loss : 3.44684481621\n",
      "Iter : 75  Test Loss : 3.26920843124\n",
      "Iter : 76  Test Loss : 1.47215199471\n",
      "Iter : 77  Test Loss : 2.85229969025\n",
      "Iter : 78  Test Loss : 2.16192293167\n",
      "Iter : 79  Test Loss : 3.52166986465\n",
      "Iter : 80  Test Loss : 2.31825828552\n",
      "Iter : 81  Test Loss : 0.992331802845\n",
      " Accuracy : 73.5\n"
     ]
    }
   ],
   "source": [
    "total_loss_i = 0\n",
    "total=0.0\n",
    "correct=0.0\n",
    "model.eval()\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    x,y=batch\n",
    "    logits = model(x.cuda())\n",
    "    loss = loss_function(logits,y.cuda())\n",
    "    total_loss_i += loss.item()\n",
    "    _, predicted = torch.max(logits.data, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted.cpu() == y).sum().item()\n",
    "    print(\"Iter : {}  Test Loss : {}\".format(i,loss.item()))\n",
    "    \n",
    "# total_loss_e+=total_loss_i/len(test_dataloader)\n",
    "print(\" Accuracy : {}\".format( (100 * (correct / total))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ground Truth:', tensor([ 4,  6,  3,  8,  2,  0,  9,  5,  6, 10, 11,  6,  6,  1,  3,  8,  6, 12,\n",
      "         3,  0,  9,  2, 12,  0,  5,  5,  5,  4,  5,  0,  1, 11]))\n",
      "('Predictions', torch.return_types.max(\n",
      "values=tensor([  8.3251, 239.5512,   5.7021,   4.3910,   3.8289,  87.0343,   6.7769,\n",
      "         42.1633, 161.4510,  11.5735,  14.7361, 197.1506, 179.5288,  86.1905,\n",
      "          7.6681,  10.0813, 188.8158,  40.2853,   7.4610, 137.8160,  13.0781,\n",
      "          5.0340,  44.5286, 106.9364,  15.0015,   8.0289,  10.4616,  10.9617,\n",
      "         12.2319,  93.6111,  96.9654,  15.8233], device='cuda:0',\n",
      "       grad_fn=<MaxBackward0>),\n",
      "indices=tensor([ 9,  6,  8,  8,  0,  0,  9,  5,  6,  3,  3,  6,  6,  1,  3,  2,  6, 12,\n",
      "         3,  0,  5,  9, 12,  0,  5,  9, 10,  9,  4,  0,  1, 11],\n",
      "       device='cuda:0')))\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(test_dataloader))\n",
    "print(\"Ground Truth:\", y)\n",
    "new_arr = x.cuda()\n",
    "preds = model(new_arr)\n",
    "print(\"Predictions\", torch.max(preds, dim =1 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled27.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
