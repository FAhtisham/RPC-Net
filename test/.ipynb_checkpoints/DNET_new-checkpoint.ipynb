{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "71f3B6CWaroo"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import load\n",
    "\n",
    "\n",
    "# numpy.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GPeh3HMaVeR_",
    "outputId": "0099c36f-3986-40f5-e3ab-759a2048d221"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: Sequences \n",
      "y: Labels \n",
      "\n",
      "x\n",
      "y\n",
      "('\\nSequences: ', (8920, 1182), '\\nLabels', (8920,))\n"
     ]
    }
   ],
   "source": [
    "# Decription of the dataset\n",
    "data = load('dataset.npz')\n",
    "lst = data.files\n",
    "\n",
    "print(\"x: Sequences \\ny: Labels \\n\")\n",
    "\n",
    "for item in lst:\n",
    "    print(item)\n",
    "\n",
    "print('\\nSequences: ',data['x'].shape,'\\nLabels', data['y'].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "H07g7P_Ibo8O"
   },
   "outputs": [],
   "source": [
    "# temp = torch.zeros((16,16), dtype=torch.int)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EFvxYncebUb7"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    One hot encoding function\n",
    "    args: 'data'\n",
    "            Type: a single np array (1182, 16)\n",
    "    return: 'New Tensor'\n",
    "            Takes the np array, finds the one hot encoding char\n",
    "            Creates a new tensor and returns it (1182, 16, 16)\n",
    "'''\n",
    "def one_hot_encoded(data): #1182*16 to 1182*16*16\n",
    "  new_tensor = torch.zeros((1182, 16,16))\n",
    "  for i in range(data.shape[0]):\n",
    "    row,col=data[i],data[i]\n",
    "    new_tensor[i,row,col]=1\n",
    "  return new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W8Tfpkj_imbL",
    "outputId": "0542b0d9-5be0-42d6-d457-587fa5596dad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 5/8920 [00:00<03:07, 47.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8920, 1182, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8920/8920 [03:44<00:00, 39.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8920, 1182, 16, 16])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert all the sequences to one hot encoded matrices of size 16*16\n",
    "\n",
    "# Load the sequences\n",
    "new_data = data['x'].astype(int)\n",
    "sequences_n = torch.zeros(new_data.shape[0], new_data.shape[1], 16,16)\n",
    "print(sequences_n.size())\n",
    "\n",
    "# Pass the sequences to one hot encoding function\n",
    "for i in tqdm(range(new_data.shape[0])):\n",
    "    sequences_n[i]=one_hot_encoded(new_data[i])\n",
    "#     if(i%200 ==0):\n",
    "#         print(i,' hogae')\n",
    "print(sequences_n.size())\n",
    "\n",
    "# Load and convert the labels to torch tensor\n",
    "labels = torch.from_numpy(data['y'].astype(long))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creation of Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6PdspGWS7Er",
    "outputId": "e9db754d-f435-4feb-cb1c-7a9fdad81e54"
   },
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "full_dataset = TensorDataset(sequences_n, labels)\n",
    "\n",
    "# Split the dataset\n",
    "train_ds, test_ds = torch.utils.data.random_split(full_dataset, (6320, 2600))\n",
    "\n",
    "# Print and confirm the dataset\n",
    "# print(train_ds, test_ds)\n",
    "# print(len(train_ds.indices), len(test_ds))\n",
    "# print(train_ds.indices, test_ds.indices)\n",
    "\n",
    "# Create the train data loader\n",
    "train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n",
    "\n",
    "# Create the test data loader\n",
    "test_dataloader = DataLoader(test_ds, batch_size=32, shuffle=False, sampler=None,\n",
    "           batch_sampler=None, num_workers=0, collate_fn=None,\n",
    "           pin_memory=False, drop_last=False, timeout=0,\n",
    "           worker_init_fn=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bia5Q7vkxYQI"
   },
   "source": [
    "# Creation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EDK1AZAjiuhi"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "    Type: RCPClass\n",
    "    The class conatains the layers of the Dense and Transition blocks\n",
    "    \n",
    "    Args: Takes the dataset (batch size, sequence_size, w, h)\n",
    "    return: Logits (Probabilities of the all 13 classes)\n",
    "'''\n",
    "\n",
    "# Create the RCPNet Class\n",
    "class RPCNet(nn.Module):\n",
    "  def __init__(self, in_channels):\n",
    "    super(RPCNet, self).__init__()\n",
    "    \n",
    "    self.in_channels = in_channels\n",
    "    self.conv1 = nn.Conv2d(in_channels=1182, out_channels=128, kernel_size=(16,17), padding=9)\n",
    "    \n",
    "    # First Dense Net Block\n",
    "    self.DenseBlock1 = DenseBlock(128)\n",
    "    \n",
    "    # Second Dense Net Block\n",
    "    self.DenseBlock2 = DenseBlock(256)\n",
    "    \n",
    "    # Third Dense Net Block\n",
    "    self.DenseBlock3 = DenseBlock(512)\n",
    "    \n",
    "    # Transition Block 1\n",
    "    self.pos_dense1 = nn.Sequential(\n",
    "          nn.BatchNorm2d(128),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(in_channels=128,out_channels=256,kernel_size=(1,1)),\n",
    "          nn.MaxPool2d(1,4))\n",
    "    \n",
    "    # Transition Block 2\n",
    "    self.pos_dense2 = nn.Sequential(\n",
    "          nn.BatchNorm2d(256),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(in_channels=256,out_channels=512,kernel_size=(1,1)),\n",
    "          nn.MaxPool2d(1,4))\n",
    "    \n",
    "    # Transition Block 3\n",
    "    self.pos_dense3 = nn.Sequential(\n",
    "          nn.BatchNorm2d(512),\n",
    "          nn.ReLU(inplace=True),\n",
    "          nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=(1,1)),\n",
    "          nn.MaxPool2d(1,4))\n",
    "    \n",
    "    # Final linear layer, can be used with a Flatten Layer \n",
    "    self.f= nn.Flatten()\n",
    "    self.Linear = nn.Linear(22528, 13)\n",
    "      \n",
    "  def forward(self, input):\n",
    "    # input = input.reshape(input.size(0), input.size(2), input.size(1))\n",
    "#     print(input.size())\n",
    "    out = self.conv1(input)\n",
    "#     print(out.size())\n",
    "    \n",
    "#     print(\"conv1 shape\", out.size())\n",
    "    weight = self.conv1.weight.data.cpu().numpy()\n",
    "#     print(\"k:\",weight.shape)\n",
    "\n",
    "    out= self.DenseBlock1(out)\n",
    "\n",
    "    out= self.pos_dense1(out)\n",
    "#     print(out.size())\n",
    "\n",
    "    out= self.DenseBlock2(out)\n",
    "#     print(out.size())\n",
    "    \n",
    "    out = self.pos_dense2(out)\n",
    "#     print(out.size())\n",
    "    \n",
    "    out= self.DenseBlock3(out)\n",
    "#     print(out.size())\n",
    "    \n",
    "    out = self.pos_dense3(out)\n",
    "#     print(out.size())\n",
    "    \n",
    "    logits = self.f(out)\n",
    "    \n",
    "    logits= self.Linear(logits)\n",
    "#     print('logits size',logits.size())\n",
    "#     print(logits)\n",
    "\n",
    "    return logits\n",
    "\n",
    "'''\n",
    "    Type: Class DenseBlock\n",
    "          Contains Batch Normalization, ReLU, Conv2D\n",
    "          \n",
    "    Input: Declaration: Num of input channels \n",
    "           Forward: Output of previous layer\n",
    "    \n",
    "    Can be used before transition blocks or other layers\n",
    "'''\n",
    "class DenseBlock(nn.Module):\n",
    "  def __init__(self, num_features):\n",
    "    super(DenseBlock, self).__init__()\n",
    "\n",
    "    self.Block1 = Block(num_features) # applies relu and BN\n",
    "    self.Block2 = Block(num_features)\n",
    "\n",
    "  def forward(self, o_input):\n",
    "\n",
    "    block1_res = self.Block1(o_input)\n",
    "#     print(\"i and c in b \",block1_res.size(), o_input.size())\n",
    "    block2_res = (torch.cat((block1_res, o_input), dim=2)) # o_inpu (concat) block1res\n",
    "#     print(block2_res.size())\n",
    "#     res = torch.cat((block1_res, block2_res, o_input), dim=2)\n",
    "#     print(res.size())\n",
    "    return  torch.cat((block1_res, block2_res, o_input), dim=2) # concat, block1_res, block2_res, o_input\n",
    "\n",
    "'''\n",
    "    Type: Block Class\n",
    "    \n",
    "    Input: Declaration: Num of input channels \n",
    "           Forward: Output of the previous layer\n",
    "           \n",
    "    return: Concatenated input and output of the BN, ReLU, Conv2D\n",
    "'''\n",
    "\n",
    "class Block(nn.Module):\n",
    "  def __init__(self, num_features):\n",
    "    super(Block, self).__init__()\n",
    "    self.num_features = num_features\n",
    "\n",
    "    self.BN = nn.BatchNorm2d(self.num_features) # try 2d\n",
    "    self.conv1 = nn.Conv2d(in_channels=self.num_features, out_channels=num_features, kernel_size=(16,17), padding=8)\n",
    "  \n",
    "  def forward(self, input):\n",
    "    res_relu = f.relu(self.BN(input))\n",
    "    res_relu_conv = self.conv1(input)\n",
    "#     print(\"w c block\",self.conv1.weight.data.cpu().numpy().shape)\n",
    "\n",
    "    return res_relu_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializaiton of Model, Optimizer, Loss function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QCPdl3wT9VPC"
   },
   "outputs": [],
   "source": [
    "model = RPCNet(1182)\n",
    "model=model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w70Z1THM1H59",
    "outputId": "59c8053b-8905-489f-f509-0b177dbe98ce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0  Train Loss : 2.59495091438\n",
      "Iter : 50  Train Loss : 2.59037566185\n",
      "Iter : 100  Train Loss : 3.42335796356\n",
      "Iter : 150  Train Loss : 2.27872872353\n",
      "Epoch  0 :   Train Loss : 2.52920232036  Accuracy : 13.6867088608\n",
      "Iter : 0  Train Loss : 2.19244933128\n",
      "Iter : 50  Train Loss : 1.80899500847\n",
      "Iter : 100  Train Loss : 2.15370464325\n",
      "Iter : 150  Train Loss : 1.82012844086\n",
      "Epoch  1 :   Train Loss : 1.9356325919  Accuracy : 27.8164556962\n",
      "Iter : 0  Train Loss : 1.91830837727\n",
      "Iter : 50  Train Loss : 1.735414505\n",
      "Iter : 100  Train Loss : 2.06462359428\n",
      "Iter : 150  Train Loss : 1.82733976841\n",
      "Epoch  2 :   Train Loss : 1.77031298117  Accuracy : 34.4778481013\n",
      "Iter : 0  Train Loss : 1.71411693096\n",
      "Iter : 50  Train Loss : 1.57152926922\n",
      "Iter : 100  Train Loss : 1.58220875263\n",
      "Iter : 150  Train Loss : 1.46863186359\n",
      "Epoch  3 :   Train Loss : 1.55194216064  Accuracy : 42.3417721519\n",
      "Iter : 0  Train Loss : 1.31476795673\n",
      "Iter : 50  Train Loss : 1.33303034306\n",
      "Iter : 100  Train Loss : 1.22522699833\n",
      "Iter : 150  Train Loss : 1.41815137863\n",
      "Epoch  4 :   Train Loss : 1.33551883878  Accuracy : 51.1392405063\n",
      "Iter : 0  Train Loss : 1.0025742054\n",
      "Iter : 50  Train Loss : 1.26563370228\n",
      "Iter : 100  Train Loss : 1.09708046913\n",
      "Iter : 150  Train Loss : 1.20758748055\n",
      "Epoch  5 :   Train Loss : 1.17621225239  Accuracy : 57.2310126582\n",
      "Iter : 0  Train Loss : 0.900400042534\n",
      "Iter : 50  Train Loss : 1.08865904808\n",
      "Iter : 100  Train Loss : 0.884644389153\n",
      "Iter : 150  Train Loss : 1.13437664509\n",
      "Epoch  6 :   Train Loss : 1.041484318  Accuracy : 61.8670886076\n",
      "Iter : 0  Train Loss : 0.915853500366\n",
      "Iter : 50  Train Loss : 0.960143208504\n",
      "Iter : 100  Train Loss : 0.868927896023\n",
      "Iter : 150  Train Loss : 0.965792417526\n",
      "Epoch  7 :   Train Loss : 0.926022308643  Accuracy : 67.4208860759\n",
      "Iter : 0  Train Loss : 0.984970152378\n",
      "Iter : 50  Train Loss : 0.802477300167\n",
      "Iter : 100  Train Loss : 0.814155936241\n",
      "Iter : 150  Train Loss : 1.0119099617\n",
      "Epoch  8 :   Train Loss : 0.835124267924  Accuracy : 70.4430379747\n",
      "Iter : 0  Train Loss : 0.586232125759\n",
      "Iter : 50  Train Loss : 0.85034018755\n",
      "Iter : 100  Train Loss : 0.678454577923\n",
      "Iter : 150  Train Loss : 0.860095441341\n",
      "Epoch  9 :   Train Loss : 0.728731848074  Accuracy : 75.3164556962\n",
      "Iter : 0  Train Loss : 0.682915210724\n",
      "Iter : 50  Train Loss : 0.720458507538\n",
      "Iter : 100  Train Loss : 0.666306555271\n",
      "Iter : 150  Train Loss : 0.747481226921\n",
      "Epoch  10 :   Train Loss : 0.647022886936  Accuracy : 77.832278481\n",
      "Iter : 0  Train Loss : 0.420474290848\n",
      "Iter : 50  Train Loss : 0.867506086826\n",
      "Iter : 100  Train Loss : 0.509988486767\n",
      "Iter : 150  Train Loss : 0.762179732323\n",
      "Epoch  11 :   Train Loss : 0.601373388987  Accuracy : 80.1265822785\n",
      "Iter : 0  Train Loss : 0.368282735348\n",
      "Iter : 50  Train Loss : 0.765770196915\n",
      "Iter : 100  Train Loss : 0.546191036701\n",
      "Iter : 150  Train Loss : 0.54112225771\n",
      "Epoch  12 :   Train Loss : 0.511115521418  Accuracy : 82.9113924051\n",
      "Iter : 0  Train Loss : 0.361047893763\n",
      "Iter : 50  Train Loss : 0.755226790905\n",
      "Iter : 100  Train Loss : 0.420039027929\n",
      "Iter : 150  Train Loss : 0.37441432476\n",
      "Epoch  13 :   Train Loss : 0.452287318241  Accuracy : 85.2373417722\n",
      "Iter : 0  Train Loss : 0.459282696247\n",
      "Iter : 50  Train Loss : 0.777669012547\n",
      "Iter : 100  Train Loss : 0.325446814299\n",
      "Iter : 150  Train Loss : 0.41893222928\n",
      "Epoch  14 :   Train Loss : 0.414581042837  Accuracy : 86.4082278481\n",
      "Iter : 0  Train Loss : 0.370603054762\n",
      "Iter : 50  Train Loss : 0.531708955765\n",
      "Iter : 100  Train Loss : 0.286120831966\n",
      "Iter : 150  Train Loss : 0.413201600313\n",
      "Epoch  15 :   Train Loss : 0.403495227335  Accuracy : 87.1518987342\n",
      "Iter : 0  Train Loss : 0.374067306519\n",
      "Iter : 50  Train Loss : 0.998943448067\n",
      "Iter : 100  Train Loss : 0.434959799051\n",
      "Iter : 150  Train Loss : 0.354440450668\n",
      "Epoch  16 :   Train Loss : 0.436397894126  Accuracy : 86.9936708861\n",
      "Iter : 0  Train Loss : 0.189441263676\n",
      "Iter : 50  Train Loss : 0.583411753178\n",
      "Iter : 100  Train Loss : 0.343994289637\n",
      "Iter : 150  Train Loss : 0.261522293091\n",
      "Epoch  17 :   Train Loss : 0.313521085476  Accuracy : 90.2056962025\n",
      "Iter : 0  Train Loss : 0.20961509645\n",
      "Iter : 50  Train Loss : 0.437032788992\n",
      "Iter : 100  Train Loss : 0.34629330039\n",
      "Iter : 150  Train Loss : 0.163054317236\n",
      "Epoch  18 :   Train Loss : 0.286813153311  Accuracy : 90.9335443038\n",
      "Iter : 0  Train Loss : 0.156656458974\n",
      "Iter : 50  Train Loss : 0.585739254951\n",
      "Iter : 100  Train Loss : 0.112389095128\n",
      "Iter : 150  Train Loss : 0.239584356546\n",
      "Epoch  19 :   Train Loss : 0.251318050868  Accuracy : 91.835443038\n",
      "Iter : 0  Train Loss : 0.19500502944\n",
      "Iter : 50  Train Loss : 0.652888715267\n",
      "Iter : 100  Train Loss : 0.130042046309\n",
      "Iter : 150  Train Loss : 0.114623263478\n",
      "Epoch  20 :   Train Loss : 0.238726979893  Accuracy : 92.167721519\n",
      "Iter : 0  Train Loss : 0.305580824614\n",
      "Iter : 50  Train Loss : 0.403051048517\n",
      "Iter : 100  Train Loss : 0.0898315608501\n",
      "Iter : 150  Train Loss : 0.0980789586902\n",
      "Epoch  21 :   Train Loss : 0.231110916445  Accuracy : 92.6582278481\n",
      "Iter : 0  Train Loss : 0.232686802745\n",
      "Iter : 50  Train Loss : 0.295353591442\n",
      "Iter : 100  Train Loss : 0.0887421518564\n",
      "Iter : 150  Train Loss : 0.253337055445\n",
      "Epoch  22 :   Train Loss : 0.216600405184  Accuracy : 93.3227848101\n",
      "Iter : 0  Train Loss : 0.190719231963\n",
      "Iter : 50  Train Loss : 0.421472787857\n",
      "Iter : 100  Train Loss : 0.045393243432\n",
      "Iter : 150  Train Loss : 0.0866963863373\n",
      "Epoch  23 :   Train Loss : 0.193067839557  Accuracy : 93.9873417722\n",
      "Iter : 0  Train Loss : 0.0671739652753\n",
      "Iter : 50  Train Loss : 0.472334086895\n",
      "Iter : 100  Train Loss : 0.0331151038408\n",
      "Iter : 150  Train Loss : 0.13855792582\n",
      "Epoch  24 :   Train Loss : 0.167538726232  Accuracy : 94.7310126582\n",
      "Iter : 0  Train Loss : 0.104983463883\n",
      "Iter : 50  Train Loss : 0.314931660891\n",
      "Iter : 100  Train Loss : 0.0945568829775\n",
      "Iter : 150  Train Loss : 0.309501230717\n",
      "Epoch  25 :   Train Loss : 0.166349636711  Accuracy : 94.7943037975\n",
      "Iter : 0  Train Loss : 0.13516689837\n",
      "Iter : 50  Train Loss : 0.315101504326\n",
      "Iter : 100  Train Loss : 0.12697263062\n",
      "Iter : 150  Train Loss : 0.0844759568572\n",
      "Epoch  26 :   Train Loss : 0.13920554101  Accuracy : 95.4430379747\n",
      "Iter : 0  Train Loss : 0.140963405371\n",
      "Iter : 50  Train Loss : 0.333302736282\n",
      "Iter : 100  Train Loss : 0.0753804370761\n",
      "Iter : 150  Train Loss : 0.112063109875\n",
      "Epoch  27 :   Train Loss : 0.135498327288  Accuracy : 95.7911392405\n",
      "Iter : 0  Train Loss : 0.0647847801447\n",
      "Iter : 50  Train Loss : 0.397564381361\n",
      "Iter : 100  Train Loss : 0.0429558083415\n",
      "Iter : 150  Train Loss : 0.185646027327\n",
      "Epoch  28 :   Train Loss : 0.180846440785  Accuracy : 94.746835443\n",
      "Iter : 0  Train Loss : 0.147237822413\n",
      "Iter : 50  Train Loss : 0.400560617447\n",
      "Iter : 100  Train Loss : 0.0598858520389\n",
      "Iter : 150  Train Loss : 0.17617417872\n",
      "Epoch  29 :   Train Loss : 0.137890104167  Accuracy : 95.8386075949\n",
      "Iter : 0  Train Loss : 0.0774250254035\n",
      "Iter : 50  Train Loss : 0.35658454895\n",
      "Iter : 100  Train Loss : 0.126582369208\n",
      "Iter : 150  Train Loss : 0.0226997435093\n",
      "Epoch  30 :   Train Loss : 0.109450072426  Accuracy : 96.5506329114\n",
      "Iter : 0  Train Loss : 0.059537306428\n",
      "Iter : 50  Train Loss : 0.294299721718\n",
      "Iter : 100  Train Loss : 0.0430121421814\n",
      "Iter : 150  Train Loss : 0.0428227186203\n",
      "Epoch  31 :   Train Loss : 0.110479793201  Accuracy : 96.4873417722\n",
      "Iter : 0  Train Loss : 0.0626966059208\n",
      "Iter : 50  Train Loss : 0.144461423159\n",
      "Iter : 100  Train Loss : 0.0188312977552\n",
      "Iter : 150  Train Loss : 0.036737293005\n",
      "Epoch  32 :   Train Loss : 0.110065153772  Accuracy : 96.6772151899\n",
      "Iter : 0  Train Loss : 0.0729375183582\n",
      "Iter : 50  Train Loss : 0.162779659033\n",
      "Iter : 100  Train Loss : 0.0330609381199\n",
      "Iter : 150  Train Loss : 0.1178740412\n",
      "Epoch  33 :   Train Loss : 0.103715210626  Accuracy : 96.8670886076\n",
      "Iter : 0  Train Loss : 0.120011359453\n",
      "Iter : 50  Train Loss : 0.201284527779\n",
      "Iter : 100  Train Loss : 0.0729729235172\n",
      "Iter : 150  Train Loss : 0.0459004193544\n",
      "Epoch  34 :   Train Loss : 0.100399659326  Accuracy : 96.6772151899\n",
      "Iter : 0  Train Loss : 0.219057321548\n",
      "Iter : 50  Train Loss : 0.0914928764105\n",
      "Iter : 100  Train Loss : 0.0177982598543\n",
      "Iter : 150  Train Loss : 0.0350898355246\n",
      "Epoch  35 :   Train Loss : 0.0921576437776  Accuracy : 97.2626582278\n",
      "Iter : 0  Train Loss : 0.104463547468\n",
      "Iter : 50  Train Loss : 0.0913891494274\n",
      "Iter : 100  Train Loss : 0.0368430763483\n",
      "Iter : 150  Train Loss : 0.109741121531\n",
      "Epoch  36 :   Train Loss : 0.0826241091421  Accuracy : 97.4367088608\n",
      "Iter : 0  Train Loss : 0.0511258468032\n",
      "Iter : 50  Train Loss : 0.162550926208\n",
      "Iter : 100  Train Loss : 0.38054651022\n",
      "Iter : 150  Train Loss : 0.2353977561\n",
      "Epoch  37 :   Train Loss : 0.0826614362874  Accuracy : 97.2626582278\n",
      "Iter : 0  Train Loss : 0.00488156080246\n",
      "Iter : 50  Train Loss : 0.210996568203\n",
      "Iter : 100  Train Loss : 0.00302447378635\n",
      "Iter : 150  Train Loss : 0.312442600727\n",
      "Epoch  38 :   Train Loss : 0.0872156946647  Accuracy : 97.2943037975\n",
      "Iter : 0  Train Loss : 0.0337426662445\n",
      "Iter : 50  Train Loss : 0.0277428925037\n",
      "Iter : 100  Train Loss : 0.0120923519135\n",
      "Iter : 150  Train Loss : 0.196369707584\n",
      "Epoch  39 :   Train Loss : 0.0713550788586  Accuracy : 97.8164556962\n",
      "Iter : 0  Train Loss : 0.00977049767971\n",
      "Iter : 50  Train Loss : 0.0873803198338\n",
      "Iter : 100  Train Loss : 0.0499253571033\n",
      "Iter : 150  Train Loss : 0.00842300057411\n",
      "Epoch  40 :   Train Loss : 0.0531077342428  Accuracy : 98.2753164557\n",
      "Iter : 0  Train Loss : 0.00474968552589\n",
      "Iter : 50  Train Loss : 0.051659733057\n",
      "Iter : 100  Train Loss : 0.0243932157755\n",
      "Iter : 150  Train Loss : 0.0121794342995\n",
      "Epoch  41 :   Train Loss : 0.0769424768485  Accuracy : 97.7215189873\n",
      "Iter : 0  Train Loss : 0.129209637642\n",
      "Iter : 50  Train Loss : 0.0554242283106\n",
      "Iter : 100  Train Loss : 0.0031181126833\n",
      "Iter : 150  Train Loss : 0.030297100544\n",
      "Epoch  42 :   Train Loss : 0.0620179522519  Accuracy : 98.1012658228\n",
      "Iter : 0  Train Loss : 0.00750620663166\n",
      "Iter : 50  Train Loss : 0.05496519804\n",
      "Iter : 100  Train Loss : 0.00171281397343\n",
      "Iter : 150  Train Loss : 0.00519773364067\n",
      "Epoch  43 :   Train Loss : 0.0626592110805  Accuracy : 98.1329113924\n",
      "Iter : 0  Train Loss : 0.0166518688202\n",
      "Iter : 50  Train Loss : 0.0338904261589\n",
      "Iter : 100  Train Loss : 0.00269465148449\n",
      "Iter : 150  Train Loss : 0.00707265734673\n",
      "Epoch  44 :   Train Loss : 0.0317735835308  Accuracy : 99.0506329114\n",
      "Iter : 0  Train Loss : 0.00424410402775\n",
      "Iter : 50  Train Loss : 0.0389465391636\n",
      "Iter : 100  Train Loss : 0.00108833611012\n",
      "Iter : 150  Train Loss : 0.0110059380531\n",
      "Epoch  45 :   Train Loss : 0.0630397845173  Accuracy : 98.0221518987\n",
      "Iter : 0  Train Loss : 0.0209704935551\n",
      "Iter : 50  Train Loss : 0.126646414399\n",
      "Iter : 100  Train Loss : 0.00800332427025\n",
      "Iter : 150  Train Loss : 0.00833040475845\n",
      "Epoch  46 :   Train Loss : 0.05060815766  Accuracy : 98.3860759494\n",
      "Iter : 0  Train Loss : 0.436278939247\n",
      "Iter : 50  Train Loss : 0.0988968759775\n",
      "Iter : 100  Train Loss : 0.0244523733854\n",
      "Iter : 150  Train Loss : 0.0124521255493\n",
      "Epoch  47 :   Train Loss : 0.0510929886891  Accuracy : 98.5917721519\n",
      "Iter : 0  Train Loss : 0.00173187255859\n",
      "Iter : 50  Train Loss : 0.0940016210079\n",
      "Iter : 100  Train Loss : 0.018178358674\n",
      "Iter : 150  Train Loss : 0.0623843669891\n",
      "Epoch  48 :   Train Loss : 0.0527533439901  Accuracy : 98.3386075949\n",
      "Iter : 0  Train Loss : 0.00796476006508\n",
      "Iter : 50  Train Loss : 0.0436306893826\n",
      "Iter : 100  Train Loss : 0.00728216767311\n",
      "Iter : 150  Train Loss : 0.0249067991972\n",
      "Epoch  49 :   Train Loss : 0.0651122287593  Accuracy : 98.164556962\n"
     ]
    }
   ],
   "source": [
    "total_loss_e = 0.0\n",
    "\n",
    "plot_loss = []\n",
    "for epoch in range(50):\n",
    "    total_loss_i = 0\n",
    "    total=0.0\n",
    "    correct=0.0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        x,y=batch\n",
    "        logits = model(x.cuda())\n",
    "        loss = loss_function(logits,y.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss_i += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += y.size(0)\n",
    "        correct += (predicted.cpu() == y).sum().item()\n",
    "        if i%50 ==0:\n",
    "            print(\"Iter : {}  Train Loss : {}\".format(i,loss.item()))\n",
    "    total_loss_e+=total_loss_i/len(train_dataloader)\n",
    "    print(\"Epoch  {} :   Train Loss : {}  Accuracy : {}\".format(epoch, total_loss_e, (100 * (correct / total))))\n",
    "    plot_loss.append(total_loss_e)\n",
    "    total_loss_e=0\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "id": "Y_-KiuH9911J"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(plot_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter : 0  Test Loss : 2.94496107101\n",
      "Iter : 1  Test Loss : 2.45704984665\n",
      "Iter : 2  Test Loss : 2.03724956512\n",
      "Iter : 3  Test Loss : 2.57082843781\n",
      "Iter : 4  Test Loss : 5.34302186966\n",
      "Iter : 5  Test Loss : 1.75534343719\n",
      "Iter : 6  Test Loss : 1.78849947453\n",
      "Iter : 7  Test Loss : 3.88453674316\n",
      "Iter : 8  Test Loss : 2.37283420563\n",
      "Iter : 9  Test Loss : 0.776594400406\n",
      "Iter : 10  Test Loss : 5.25816011429\n",
      "Iter : 11  Test Loss : 1.26096749306\n",
      "Iter : 12  Test Loss : 3.16063666344\n",
      "Iter : 13  Test Loss : 3.2196290493\n",
      "Iter : 14  Test Loss : 2.58067417145\n",
      "Iter : 15  Test Loss : 2.39291763306\n",
      "Iter : 16  Test Loss : 2.45247483253\n",
      "Iter : 17  Test Loss : 2.16455054283\n",
      "Iter : 18  Test Loss : 2.19094610214\n",
      "Iter : 19  Test Loss : 2.048869133\n",
      "Iter : 20  Test Loss : 3.94749498367\n",
      "Iter : 21  Test Loss : 1.6768206358\n",
      "Iter : 22  Test Loss : 1.79274606705\n",
      "Iter : 23  Test Loss : 2.600055933\n",
      "Iter : 24  Test Loss : 4.39622020721\n",
      "Iter : 25  Test Loss : 2.1419699192\n",
      "Iter : 26  Test Loss : 1.97283899784\n",
      "Iter : 27  Test Loss : 2.50723648071\n",
      "Iter : 28  Test Loss : 2.83373165131\n",
      "Iter : 29  Test Loss : 4.01042556763\n",
      "Iter : 30  Test Loss : 1.93126702309\n",
      "Iter : 31  Test Loss : 2.10212492943\n",
      "Iter : 32  Test Loss : 1.87583470345\n",
      "Iter : 33  Test Loss : 2.10917448997\n",
      "Iter : 34  Test Loss : 3.12763953209\n",
      "Iter : 35  Test Loss : 1.67030858994\n",
      "Iter : 36  Test Loss : 3.53262114525\n",
      "Iter : 37  Test Loss : 2.36229538918\n",
      "Iter : 38  Test Loss : 2.93284916878\n",
      "Iter : 39  Test Loss : 1.68035924435\n",
      "Iter : 40  Test Loss : 1.74189507961\n",
      "Iter : 41  Test Loss : 1.2951823473\n",
      "Iter : 42  Test Loss : 1.25837206841\n",
      "Iter : 43  Test Loss : 3.03675603867\n",
      "Iter : 44  Test Loss : 0.61016112566\n",
      "Iter : 45  Test Loss : 1.95963704586\n",
      "Iter : 46  Test Loss : 3.92857336998\n",
      "Iter : 47  Test Loss : 2.44339394569\n",
      "Iter : 48  Test Loss : 0.966798007488\n",
      "Iter : 49  Test Loss : 1.14876639843\n",
      "Iter : 50  Test Loss : 0.98660248518\n",
      "Iter : 51  Test Loss : 2.2427110672\n",
      "Iter : 52  Test Loss : 1.8940217495\n",
      "Iter : 53  Test Loss : 2.55012249947\n",
      "Iter : 54  Test Loss : 2.63824510574\n",
      "Iter : 55  Test Loss : 2.96804404259\n",
      "Iter : 56  Test Loss : 2.61360836029\n",
      "Iter : 57  Test Loss : 1.67857384682\n",
      "Iter : 58  Test Loss : 4.36670875549\n",
      "Iter : 59  Test Loss : 1.74821519852\n",
      "Iter : 60  Test Loss : 2.72262907028\n",
      "Iter : 61  Test Loss : 3.17984676361\n",
      "Iter : 62  Test Loss : 1.38008284569\n",
      "Iter : 63  Test Loss : 1.7200217247\n",
      "Iter : 64  Test Loss : 2.72861289978\n",
      "Iter : 65  Test Loss : 2.42266631126\n",
      "Iter : 66  Test Loss : 2.41111946106\n",
      "Iter : 67  Test Loss : 1.10315811634\n",
      "Iter : 68  Test Loss : 2.28860735893\n",
      "Iter : 69  Test Loss : 2.9467446804\n",
      "Iter : 70  Test Loss : 1.96052312851\n",
      "Iter : 71  Test Loss : 1.35037541389\n",
      "Iter : 72  Test Loss : 2.26676988602\n",
      "Iter : 73  Test Loss : 0.978027701378\n",
      "Iter : 74  Test Loss : 2.597884655\n",
      "Iter : 75  Test Loss : 2.57438516617\n",
      "Iter : 76  Test Loss : 1.85042774677\n",
      "Iter : 77  Test Loss : 4.92865943909\n",
      "Iter : 78  Test Loss : 2.4026632309\n",
      "Iter : 79  Test Loss : 3.08066225052\n",
      "Iter : 80  Test Loss : 3.69449090958\n",
      "Iter : 81  Test Loss : 2.90404558182\n",
      " Accuracy : 71.9230769231\n"
     ]
    }
   ],
   "source": [
    "total_loss_i = 0\n",
    "total=0.0\n",
    "correct=0.0\n",
    "model.eval()\n",
    "for i, batch in enumerate(test_dataloader):\n",
    "    x,y=batch\n",
    "    logits = model(x.cuda())\n",
    "    loss = loss_function(logits,y.cuda())\n",
    "    total_loss_i += loss.item()\n",
    "    _, predicted = torch.max(logits.data, 1)\n",
    "    total += y.size(0)\n",
    "    correct += (predicted.cpu() == y).sum().item()\n",
    "    print(\"Iter : {}  Test Loss : {}\".format(i,loss.item()))\n",
    "    \n",
    "# total_loss_e+=total_loss_i/len(test_dataloader)\n",
    "print(\" Accuracy : {}\".format( (100 * (correct / total))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Ground Truth:', tensor([ 4,  6,  3,  8,  2,  0,  9,  5,  6, 10, 11,  6,  6,  1,  3,  8,  6, 12,\n",
      "         3,  0,  9,  2, 12,  0,  5,  5,  5,  4,  5,  0,  1, 11]))\n",
      "('Predictions', torch.return_types.max(\n",
      "values=tensor([14.4684, 85.1889,  9.1602,  8.8477,  4.4606, 36.2412,  6.6645, 22.4495,\n",
      "        47.1942, 22.3329, 17.2882, 66.4394, 88.0309, 80.9064, 14.3579,  5.3693,\n",
      "        91.5169, 37.7294, 11.4029, 55.8283, 18.3636, 18.2256, 37.0522, 48.3913,\n",
      "        11.2702, 14.9665,  8.7802, 16.0170, 16.6690, 35.1423, 78.4830, 23.1582],\n",
      "       device='cuda:0', grad_fn=<MaxBackward0>),\n",
      "indices=tensor([ 4,  6,  8,  8, 11,  0,  4,  5,  6,  4, 11,  6,  6,  1,  3,  2,  6, 12,\n",
      "         3,  0, 10,  2, 12,  0,  5,  4,  5,  9,  5,  0,  1, 11],\n",
      "       device='cuda:0')))\n"
     ]
    }
   ],
   "source": [
    "x,y = next(iter(test_dataloader))\n",
    "print(\"Ground Truth:\", y)\n",
    "new_arr = x.cuda()\n",
    "preds = model(new_arr)\n",
    "print(\"Predictions\", torch.max(preds, dim =1 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled27.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
