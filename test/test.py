# -*- coding: utf-8 -*-
"""Untitled27.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TYlccr-U4cB73uil1E7vTRNxLL2G3heZ
"""

import torch
import torch.nn as nn
import torch.nn.functional as f
from torch.utils.data import TensorDataset, DataLoader

import numpy as np
from numpy import load
import sys
import numpy

def one_hot_encoded(data): #1182*16 to 1182*16*16
  new_tensor = torch.zeros((1182, 16,16), dtype=torch.int)
  for i in range(data.shape[0]):
    row,col=data[i],data[i]
    # temp2=temp
    # temp2[row][col]=1
    new_tensor[i,row,col]=1
    # new_tensor[i]=temp2
  return new_tensor


"""# Creating the Model"""

# class RPCNet(nn.Module):
#   def __init__(self, in_channels):
#     super(RPCNet, self).__init__()
#     self.in_channels = in_channels

#     self.conv1 = nn.Conv1d(in_channels=16, out_channels=128, kernel_size=(16,17), padding=1)
#     self.DenseBlock1 = DenseBlock(128) # applies relu and BN

#     # Conv 128, 16*17

#   def forward(self, input):
#     input = input.reshape(input.size(0), input.size(2), input.size(1))
#     print(input.size())
#     res = self.conv1(input)
#     print("conv1 shape", res.size())
#     weight = self.conv1.weight.data.numpy()
#     print("k:",weight.shape)

#     res2= self.DenseBlock1(res)
#     print(res.size(), res2.size())
    
#     # print(weight[0,:,:])
#     # print("o:",res.size())
    

# class DenseBlock(nn.Module):
#   def __init__(self, num_features):
#     super(DenseBlock, self).__init__()
#     self.num_features = num_features
#     # self.in_channels = in_channels 

#     self.BN = nn.BatchNorm1d(self.num_features) # try 2d
#     self.conv1 = nn.Conv1d(in_channels=self.num_features, out_channels=128, kernel_size=(17), padding=1)
  
#   def forward(self, input):
#     res_relu = F.relu(self.BN(input))
#     res_relu_conv = self.conv1(input)
#     print("w",self.conv1.weight.data.numpy().shape)

#     return res_relu_conv

class RPCNet(nn.Module):
  def __init__(self, in_channels):
    super(RPCNet, self).__init__()
    self.in_channels = in_channels

    self.conv1 = nn.Conv2d(in_channels=1182, out_channels=128, kernel_size=(16,17), padding=100)
    self.DenseBlock1 = DenseBlock(128)
    self.pos_dense1 = nn.Sequential(
          nn.BatchNorm2d(128),
          nn.ReLU(),
          nn.Conv2d(256,1,1),
          nn.MaxPool2d(1,4))
      
    
    self.DenseBlock2 = DenseBlock(128)

  def forward(self, input):
    # input = input.reshape(input.size(0), input.size(2), input.size(1))
    print(input.size())
    out = self.conv1(input)
    print(out.size())
    
    # print("conv1 shape", res.size())
    # weight = self.conv1.weight.data.numpy()
    # print("k:",weight.shape)

    # out= self.DenseBlock1(out)

    # out= self.pos_dense1(out)

    # # out= self.DenseBlock2(out)

    # print(res.size(), res2.size())
    
    # print(weight[0,:,:])
    # print("o:",res.size())
    
class DenseBlock(nn.Module):
  def __init__(self, num_features):
    super(DenseBlock, self).__init__()

    self.Block1 = Block(128) # applies relu and BN
    self.Block2 = Block(128)

  def forward(self, o_inp):

    block1_res = self.Block1(o_input)
    block2_res = (torch.cat((block1_res, o_input), dim=1)) # o_inpu (concat) block1res

    return  torch.cat((block1_res, block2_res, o_input), dim=1) # concat, block1_res, block2_res, o_input

class Block(nn.Module):
  def __init__(self, num_features):
    super(Block, self).__init__()
    self.num_features = num_features
    # self.in_channels = in_channels 

    self.BN = nn.BatchNorm2d(self.num_features) # try 2d
    self.conv1 = nn.Conv2d(in_channels=self.num_features, out_channels=128, kernel_size=(17), padding=1)
  
  def forward(self, input):
    res_relu = F.relu(self.BN(input))
    res_relu_conv = self.conv1(input)
    print("w",self.conv1.weight.data.numpy().shape)

    return res_relu_conv


#################################################################################
# Decription of the dataset
data = load('dataset.npz')
lst = data.files
print("x: Sequences \ny: Labels \n")
for item in lst:
    print(item)
print('\nSequences: ',data['x'].shape,'\nLabels', data['y'].shape)





new_data = data['x'].astype(int)


sequences_n = torch.zeros(new_data.shape[0], new_data.shape[1], 16,16 )
print(sequences_n.size())
for i in range(new_data.shape[0]):
  sequences_n[i]=one_hot_encoded(new_data[i])

print(sequences_n.size())

labels = torch.from_numpy(data['y'])



full_dataset = TensorDataset(sequences_n, labels)
train_ds, test_ds = torch.utils.data.random_split(full_dataset, (6320, 2600))

print(train_ds, test_ds)
print(len(train_ds.indices), len(test_ds))
print(train_ds.indices, test_ds.indices)


train_dataloader = DataLoader(train_ds, batch_size=32, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None, prefetch_factor=2,
           persistent_workers=False)


test_dataloader = DataLoader(test_ds, batch_size=32, shuffle=False, sampler=None,
           batch_sampler=None, num_workers=0, collate_fn=None,
           pin_memory=False, drop_last=False, timeout=0,
           worker_init_fn=None, prefetch_factor=2,
           persistent_workers=False)






######################################
model = RPCNet(1182)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_function = nn.CrossEntropyLoss()
model = model.cuda()

# for i in range(num_epochs = 10):
for i, batch in enumerate(train_dataloader):
  x,y=batch
  x=x.cuda()
  # logits = model(x)
  model(x)
  # print(x.size(), y.size())
  
  # loss = loss_function(y, logits)
  # optimizer.zero_grad()
  # loss.backward()
  # optimizer.step()
  break

